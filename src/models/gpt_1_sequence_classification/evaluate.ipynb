{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\n",
    "    os.path.abspath(\n",
    "        os.path.join(os.path.dirname(os.path.abspath(\"__file__\")), \"../../..\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from textwrap import dedent\n",
    "from typing import Dict, Literal\n",
    "\n",
    "from datasets import DatasetDict, load_dataset, Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "from src.shared.config import settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_GPT_ID = \"openai-gpt\"\n",
    "FINETUNED_HF_GPT_ID = f\"{settings.hf_user_name}/{settings.gpt_1_sequence_classification}\"\n",
    "\n",
    "\n",
    "def tokenizer_init():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(HF_GPT_ID)\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\"pad_token\": \"<pad>\"}\n",
    "    )  # gpt-1 tokenizer lacks this by default\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def model_init():\n",
    "    tokenizer = tokenizer_init()\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(FINETUNED_HF_GPT_ID, num_labels=8)\n",
    "    model.resize_token_embeddings(\n",
    "        len(tokenizer), mean_resizing=False\n",
    "    )  # extend the embedding layer to handle padding token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    return model.to(settings.device)\n",
    "\n",
    "\n",
    "def format_dataset_row(\n",
    "    row, label_mapping: Dict[str, int]\n",
    ") -> Dict[Literal[\"text\"], str]:\n",
    "    row = dict(row)\n",
    "    from_account = row.pop(\"from_account\")\n",
    "    formatted = dedent(\n",
    "        f\"\"\"\n",
    "        Transaction\n",
    "        -----------\n",
    "        Description: {row['description']}\n",
    "        Amount: {row['amount']}\n",
    "        Category: {row['category']} (Source: {row['category_source']})\n",
    "        Transaction Date: {row['transaction_date']}\n",
    "        Day of Week: {row['day_of_week']}\n",
    "        Card: {row['card']}\n",
    "\n",
    "        Question: Which account initiated this transaction?\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    ).strip()\n",
    "    return {\"text\": formatted, \"labels\": label_mapping[from_account]}\n",
    "\n",
    "def test_dataset_init(tokenizer) -> DatasetDict:\n",
    "    dataset = load_dataset(f\"{settings.hf_user_name}/{settings.hf_dataset_repo_name}\")\n",
    "\n",
    "    unique_from_accounts = sorted(set(\n",
    "        dataset[\"train\"][\"from_account\"] + dataset[\"test\"][\"from_account\"]\n",
    "    ))\n",
    "    label_mapping = {account: idx for idx, account in enumerate(unique_from_accounts)}\n",
    "    print(f'label_mapping: {label_mapping}')\n",
    "    format_dataset_row_partial = partial(\n",
    "        format_dataset_row,\n",
    "        label_mapping=label_mapping,\n",
    "    )\n",
    "\n",
    "    dataset[\"test\"] = dataset[\"test\"].map(format_dataset_row_partial)\n",
    "    del dataset[\"train\"]\n",
    "\n",
    "    remove_columns = ['transaction_date', 'description', 'amount', 'category', 'category_source', 'card', 'day_of_week']\n",
    "    dataset = dataset.map(\n",
    "        lambda batch: tokenizer(batch[\"text\"]),\n",
    "        batched=True,\n",
    "        remove_columns=remove_columns,\n",
    "    )\n",
    "    dataset.set_format(\"pt\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_mapping: {'Assets:Discover:Furniture': 0, 'Assets:Discover:FutureWants': 1, 'Assets:Discover:Main:Needs:Gas': 2, 'Assets:Discover:Main:Needs:Groceries': 3, 'Assets:Discover:Main:Needs:Monthly': 4, 'Assets:Discover:Main:Needs:Other': 5, 'Assets:Discover:Main:Wants:Monthly': 6, 'Assets:Discover:Main:Wants:Other': 7}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.927461139896373"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/tmp/eval\",\n",
    "    per_device_eval_batch_size=64,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_init(),\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer_init()),\n",
    ")\n",
    "\n",
    "dataset = test_dataset_init(tokenizer_init())\n",
    "\n",
    "predictions = trainer.predict(dataset[\"test\"])\n",
    "actual = predictions.label_ids\n",
    "predicted = predictions.predictions.argmax(axis=-1)\n",
    "\n",
    "# Calculate accuracy using numpy for better performance\n",
    "accuracy = (predicted == actual).mean()\n",
    "accuracy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction\n",
      "-----------\n",
      "Description: O DONELL ACE HARDWARE DES MOINES IA\n",
      "Amount: 23.19\n",
      "Category: Home Improvement (Source: Discover)\n",
      "Transaction Date: 2023-03-22\n",
      "Day of Week: Wednesday\n",
      "Card: Discover It Chrome\n",
      "\n",
      "Question: Which account initiated this transaction?\n",
      "Answer:\n",
      "\n",
      "Assets:Discover:Main:Needs:Other\n",
      "\n",
      "Assets:Discover:Main:Needs:Other\n"
     ]
    }
   ],
   "source": [
    "instance = 0\n",
    "example = dataset[\"test\"][instance]\n",
    "\n",
    "instance_dataset = Dataset.from_dict({\n",
    "    'input_ids': [example['input_ids']],\n",
    "    'attention_mask': [example['attention_mask']],\n",
    "    'labels': [example['labels']]\n",
    "})\n",
    "\n",
    "unique_from_accounts = sorted(set(\n",
    "    dataset[\"test\"][\"from_account\"]\n",
    "))\n",
    "label_mapping = {account: idx for idx, account in enumerate(unique_from_accounts)}\n",
    "reverse_mapping = {idx: account for account, idx in label_mapping.items()}\n",
    "\n",
    "prediction = trainer.predict(instance_dataset)\n",
    "predicted_class = prediction.predictions.argmax(axis=-1)[0]\n",
    "\n",
    "actual = reverse_mapping[example['labels'].item()]\n",
    "prediction = reverse_mapping[predicted_class]\n",
    "\n",
    "print(example[\"text\"], actual, prediction, sep=\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
