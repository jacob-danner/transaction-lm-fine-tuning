{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\n",
    "    os.path.abspath(\n",
    "        os.path.join(os.path.dirname(os.path.abspath(\"__file__\")), \"../../..\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "from textwrap import dedent\n",
    "from typing import Dict, Literal\n",
    "\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, EarlyStoppingCallback, Trainer, TrainingArguments\n",
    "\n",
    "from src.shared.config import settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_GPT_ID = \"openai-gpt\"\n",
    "FINETUNED_HF_GPT_ID = f\"{settings.hf_user_name}/{settings.gpt_1_sequence_classification}\"\n",
    "\n",
    "\n",
    "def tokenizer_init():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(HF_GPT_ID)\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\"pad_token\": \"<pad>\"}\n",
    "    )  # gpt-1 tokenizer lacks this by default\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def model_init():\n",
    "    tokenizer = tokenizer_init()\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(FINETUNED_HF_GPT_ID, num_labels=9)\n",
    "    model.resize_token_embeddings(\n",
    "        len(tokenizer), mean_resizing=False\n",
    "    )  # extend the embedding layer to handle padding and eos tokens\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    return model.to(settings.device)\n",
    "\n",
    "\n",
    "def format_dataset_row(\n",
    "    row, unique_from_accounts: set, label_mapping: Dict[str, int]\n",
    ") -> Dict[Literal[\"text\"], str]:\n",
    "    row = dict(row)\n",
    "    from_account = row.pop(\"from_account\")\n",
    "    formatted = dedent(\n",
    "    f\"\"\"\n",
    "        <possible accounts>{', '.join(unique_from_accounts)}</possible accounts>\n",
    "        <transaction>{json.dumps(row)}</transaction>\n",
    "        which account did this transaction come from?\n",
    "    \"\"\"\n",
    "    ).strip()\n",
    "    return {\"text\": formatted, \"labels\": label_mapping[from_account]}\n",
    "\n",
    "\n",
    "def test_dataset_init(tokenizer) -> DatasetDict:\n",
    "    dataset = load_dataset(f\"{settings.hf_user_name}/{settings.hf_dataset_repo_name}\")\n",
    "\n",
    "    unique_from_accounts = set(\n",
    "        dataset[\"train\"][\"from_account\"] + dataset[\"test\"][\"from_account\"]\n",
    "    )\n",
    "    label_mapping = {account: idx for idx, account in enumerate(unique_from_accounts)}\n",
    "    format_dataset_row_partial = partial(\n",
    "        format_dataset_row,\n",
    "        unique_from_accounts=unique_from_accounts,\n",
    "        label_mapping=label_mapping,\n",
    "    )\n",
    "\n",
    "    dataset[\"test\"] = dataset[\"test\"].map(format_dataset_row_partial)\n",
    "    del dataset[\"train\"]\n",
    "\n",
    "    remove_columns = [\n",
    "        \"amount\",\n",
    "        \"month\",\n",
    "        \"day\",\n",
    "        \"year\",\n",
    "        \"vendor\",\n",
    "        \"from_account\",\n",
    "        \"text\"\n",
    "    ]\n",
    "    dataset = dataset.map(\n",
    "        lambda batch: tokenizer(batch[\"text\"]),\n",
    "        batched=True,\n",
    "        remove_columns=remove_columns,\n",
    "    )\n",
    "    dataset.set_format(\"pt\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ea4cf5a24b4f0d9f325bc08485df93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff392800bb74cc0b4d08c43542e4504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/466M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf84a598a1a4f1ebd2d802df0443492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/187 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a26816d0c3e4dd4b2f69a085140425a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/187 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.18181818181818182"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/tmp/eval\",\n",
    "    per_device_eval_batch_size=64,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_init(),\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer_init()),\n",
    ")\n",
    "\n",
    "dataset = test_dataset_init(tokenizer_init())\n",
    "\n",
    "predictions = trainer.predict(dataset[\"test\"])\n",
    "actual = predictions.label_ids\n",
    "predictions = predictions.predictions.argmax(-1)\n",
    "\n",
    "accuracy = sum(1 for pred, act in zip(predictions, actual) if pred == act) / len(predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<possible accounts>Assets:Discover:Main:Needs:Groceries, Assets:Discover:Main:Needs:Monthly, Assets:Discover:Main:Wants:Monthly, Assets:Discover:Main:Needs:Other, Assets:Discover:Travel, Assets:Discover:FutureWants, Assets:Discover:Main:Wants:Other, Assets:Discover:Main:Needs:Gas, Assets:Discover:Furniture</possible accounts>\n",
      "<transaction>{\"amount\": 89.99, \"month\": 6, \"day\": 4, \"year\": 2023, \"vendor\": \"WALMART\"}</transaction>\n",
      "which account did this transaction come from?\n",
      "answer:\n",
      "\n",
      "Assets:Discover:Main:Needs:Groceries\n",
      "\n",
      "Assets:Discover:Main:Needs:Groceries\n"
     ]
    }
   ],
   "source": [
    "example = dataset[\"test\"][\"text\"][21]\n",
    "actual = dataset[\"test\"][\"from_account\"][21]\n",
    "prediction = pipeline(example)\n",
    "\n",
    "print(example, actual, prediction, sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIGPTForSequenceClassification(\n",
       "  (transformer): OpenAIGPTModel(\n",
       "    (tokens_embed): Embedding(40479, 768)\n",
       "    (positions_embed): Embedding(512, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
